@article{chiassonDoesHumanProfessor2024,
  title = {Does the Human Professor or Artificial Intelligence ({{AI}}) Offer Better Explanations to Students? {{Evidence}} from Three within-Subject Experiments},
  shorttitle = {Does the Human Professor or Artificial Intelligence ({{AI}}) Offer Better Explanations to Students?},
  author = {Chiasson, Rebekah M. and , Alan K., Goodboy and , Megan A., Vendemia and , Nathaniel, Beer and , Gracyn C., Meisz and , Laken, Cooper and , Alyssa, Arnold and , Austin, Lincoski and , William, George and , Cole, Zuckerman and family=Schrout, given=Jessica, prefix=and, useprefix=true},
  date = {2024-10-01},
  journaltitle = {Communication Education},
  volume = {73},
  number = {4},
  pages = {343--370},
  publisher = {NCA Website},
  issn = {0363-4523},
  doi = {10.1080/03634523.2024.2398105},
  url = {https://doi.org/10.1080/03634523.2024.2398105},
  urldate = {2025-04-06},
  abstract = {Three within-subject experiments were conducted by providing students with answers to content questions across different subject matters (a definition, explanation, and example) offered by a human professor (subject-matter expert) versus generative artificial intelligence (ChatGPT). In a randomized order, students read both the expert and ChatGPT’s responses (both were de-identified and declared to be “professors,” so students were not aware one was artificial intelligence), rated both explanations on teaching clarity and competence, and then reported on their affect toward the content and situational interest. Study 1 (interpersonal communication content) revealed no significant differences in repeated measure ratings comparing the expert versus ChatGPT. However, in Study 2 (business communication content) and Study 3 (instructional communication content), compared with the expert, ChatGPT (impersonating a professor) was rated by the same students as higher in teaching clarity and competence, and it generated more student affect and situational interest. In Study 2 and Study 3, a within-subjects mediation analysis revealed that ChatGPT generated more student affect toward the content through the clarity in responses it provided to students.},
  keywords = {AI,ChatGPT,Education,generative artificial intelligence,higher education,instructional communication,machine teacher}
}

@online{googleGoogleNotebookLMNote,
  title = {Google {{NotebookLM}} | {{Note Taking}} \& {{Research Assistant Powered}} by {{AI}}},
  author = {{Google}},
  url = {https://notebooklm.google/},
  urldate = {2025-04-06},
  abstract = {Use the power of AI for quick summarization and note taking, NotebookLM is your powerful virtual research assistant rooted in information you can trust.},
  langid = {american},
  keywords = {AI},
  file = {C:\Users\Chris\Zotero\storage\32JTW9XS\notebooklm.google.html}
}

@online{openaiIntroducingOpenAIO1,
  title = {Introducing {{OpenAI}} O1},
  author = {{OpenAI}},
  url = {https://openai.com/index/introducing-openai-o1-preview/},
  urldate = {2025-04-06},
  abstract = {Introducing OpenAI o1},
  langid = {american},
  file = {C:\Users\Chris\Zotero\storage\LHJG4E7U\introducing-openai-o1-preview.html}
}

@online{parliamentofaustraliaInquiryUseGenerative,
  type = {text},
  title = {Inquiry into the Use of Generative Artificial Intelligence in the {{Australian}} Education System},
  author = {{Parliament of Australia,}},
  publisher = {corporateName=Commonwealth Parliament; address=Parliament House, Canberra, ACT, 2600; contact=+61 2 6277 7111},
  url = {https://www.aph.gov.au/Parliamentary_Business/Committees/House/Employment_Education_and_Training/AIineducation},
  urldate = {2025-04-06},
  abstract = {The House Standing Committee on Employment, Education and Training adopted an inquiry into the use of generative artificial intelligence in the Australian education system on 24 May 2023 following a referral from the Minister for Education, the Hon Jason Clare MP. The Committee},
  langid = {australian},
  annotation = {Last Modified: 2024-09-10},
  file = {C:\Users\Chris\Zotero\storage\PP4B92JS\AIineducation.html}
}

@article{RN1761,
  type = {Journal Article},
  title = {Autonomous Cars: {{Self-driving}} the New Auto Industry Paradigm},
  author = {Shanker, Ravi and Jonas, Adam and Devitt, Scott and Huberty, Katy and Flannery, Simon and Greene, William and Swinburne, Benjamin and Locraft, Gregory and Wood, Adam and Weiss, Keith},
  date = {2013},
  journaltitle = {Morgan Stanley blue paper},
  pages = {1--109}
}

@online{tertiaryeducationqualityandstandardsagencyArtificialIntelligenceRequest,
  title = {Artificial Intelligence Request for Information – next Steps | {{Tertiary Education Quality}} and {{Standards Agency}}},
  author = {{Tertiary Education Quality and Standards Agency,}},
  url = {https://www.teqsa.gov.au/about-us/news-and-events/latest-news/artificial-intelligence-request-information-next-steps},
  urldate = {2025-04-06},
  file = {C:\Users\Chris\Zotero\storage\UPS4JSJA\artificial-intelligence-request-information-next-steps.html}
}

@online{vaswaniAttentionAllYou2023a,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-02},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2025-04-06},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {AI,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Chris\\Zotero\\storage\\NDSUQTXF\\Vaswani et al. - 2023 - Attention Is All You Need.pdf;C\:\\Users\\Chris\\Zotero\\storage\\MR4I2TJQ\\1706.html}
}
