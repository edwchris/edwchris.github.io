<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-03-15">

<title>AI in Sonography – Chris Edwards</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-69T2DQZDC7"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-69T2DQZDC7', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Chris Edwards</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog/index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/edwchris"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">AI in Sonography</h1>
            <p class="subtitle lead">The original version of this article appeared in the Nov 2024 edition of the ASA - Soundefects News.</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Sonography</div>
                <div class="quarto-category">Artifical Intelligence</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 15, 2025</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">April 6, 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#the-rise-of-ai" id="toc-the-rise-of-ai" class="nav-link active" data-scroll-target="#the-rise-of-ai">The Rise of AI</a></li>
  <li><a href="#professional-context-ais-role-in-sonography-practice" id="toc-professional-context-ais-role-in-sonography-practice" class="nav-link" data-scroll-target="#professional-context-ais-role-in-sonography-practice">Professional Context – AI’s Role in Sonography Practice</a></li>
  <li><a href="#sonography-education-balancing-ai-tools-and-integrity" id="toc-sonography-education-balancing-ai-tools-and-integrity" class="nav-link" data-scroll-target="#sonography-education-balancing-ai-tools-and-integrity">Sonography Education – Balancing AI Tools and Integrity</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<section id="the-rise-of-ai" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-rise-of-ai">The Rise of AI</h2>
<p>Lately, it’s been hard to escape discussions about artificial intelligence (AI) and its potential to reshape our lives. Conversations often focus on its influence on work and whether it will meaningfully impact the role of sonographers. How will it affect education and the training of future professionals? Beyond our field, the media is full of warnings about rogue AIs fuelling social unrest by spreading misinformation or aiding criminals in creating elaborate scams. Recently, AI-generated content – images and videos – has flooded the internet. While some of it is amusing, much of it has little value, earning the nickname ‘slop’ or ‘AI-generated spam’.</p>
<p>The recent surge in public interest in AI can be traced back to the release of OpenAI’s ChatGPT in November 2022. However, the technology behind it – specifically the transformer, the ‘T’ in ChatGPT – has been around since 2017<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Over the past year, there has been a rapid rise in user-friendly interfaces, making the technology accessible to a much broader section of the public. Today, numerous tools allow users to generate high quality text and create images and videos with an ever-expanding range of possibilities. In September, Google released an audio add-on to <a href="https://notebooklm.google/">NotebookLM</a>, their personalised AI collaborator, allowing users to create a podcast from text-based inputs. Could this be a handy study tool? Imagine students uploading their ultrasound physics notes and listening to AI-generated podcasters discuss parallel beamforming on their commute, complete with ‘ums’ and ‘ahs’. There’s even evidence suggesting that, in specific contexts, AI can explain content more effectively than professors<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Vaswani A. Attention is all you need. <em>Advances in Neural Information Processing Systems</em>, 2017. <a href="https://doi.org/10.48550/arXiv.1706.03762" class="uri">https://doi.org/10.48550/arXiv.1706.03762</a></p></div><div id="fn2"><p><sup>2</sup>&nbsp;Chiasson RM, et al.&nbsp;Does the human professor or artificial intelligence (AI) offer better explanations to students? <em>Communication Education</em>, 2024; 1–28. <a href="https://doi.org/10.1080/03634523.2024.2398105" class="uri">https://doi.org/10.1080/03634523.2024.2398105</a></p></div></div><p>Applications that use transformers will continue to evolve. A transformer is an AI model trained via machine learning on vast amounts of data. The training employs a technique called ‘backpropagation’, a type of feedback loop that improves the model’s accuracy. During backpropagation, the model calculates the error between predicted and actual outputs and then uses this error to adjust parameters to improve model performance over time. This could be text in the case of large language models (LLMs) or images and videos.</p>
<p>Some of the work currently done at QUT, analysing ultrasound images, uses a type of transformer called a Swin Transformer. The Swin Transformer, short for Shifted Window Transformer, is designed to deal with the complexity of image processing. Unlike text, where information flows sequentially, images are spatially located. For example, in medical images, the diagnosis isn’t just in individual pixels but in how these pixels relate to one another across the entire visual field. Ultrasound images are particularly complex and include various acoustic features that relate to one another, such as their echotexture and position in the image. The correct characterisation of an image may even include features that extend across frames.</p>
<p>Once training is complete, the model doesn’t ‘relearn’ each time it gets a new prompt; instead, it leverages its pre-learned knowledge to respond in real time. It does this quickly because it’s designed to process information in parallel rather than sequentially, making it highly efficient. Some commercial ultrasound systems have already deployed this technology with onboard AI assistants to highlight particular anatomy or the correct scan plane. For example, targeting a nerve before a pain block or identifying fetal structures during obstetric scanning.</p>
</section>
<section id="professional-context-ais-role-in-sonography-practice" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="professional-context-ais-role-in-sonography-practice">Professional Context – AI’s Role in Sonography Practice</h2>
<p>Predicting how this technology will ultimately impact the sonographer’s role is challenging. Some tools may work in the background to improve image quality but others will require direct interactions with the sonographer. Suppose one were to view AI from a purely technical or capability standpoint – for example, a system that employs machine learning to predict an output from text, images, audio or video. With the current level of AI technology, it is perhaps reasonable to suggest that, in a short period, an ultrasound system will be equipped with advanced image analysis tools, possibly automatically labelling anatomy and highlighting a range of pathologies. Further, it may be able to analyse sets of images or videos and compute a report. Regarding audio, perhaps our machines may be equipped with an inbuilt audio interface that will interact with the patient directly, reassuring them that everything is in hand and answering any questions they may have in a professional, empathetic tone. The system may even prompt you if the patient mentions some relevant clinical history, conveniently prefilling these details into an electronic worksheet before you have left the room. What about wholly automated systems using advanced robotics?</p>
<p>The world was promised the widespread adoption of driverless cars; Morgan Stanley in 2013 predicted this would occur in 2026<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. Current predictions have now gone beyond another two decades. This is partly due to the messiness and unpredictability of real life and the difficulty of designing models to cope. In the case of driverless cars, sudden changes in the weather, roadworks, and, of course, other human drivers engage in irrational behaviour on the road. Healthcare is similarly complex, filled with ambiguity, human emotion, and multiple competing elements. AI models must be robust enough to handle these realities. In sonography, this means dealing with everyday realities like extremes of body habitus, low patient acuity, and challenging patient interactions. Sonographers currently address these issues using techniques such as non-standardised plans and advanced communication skills tailored to each patient’s condition and clinical history.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Shanker R, et al.&nbsp;Autonomous cars: Self-driving the new auto industry paradigm. <em>Morgan Stanley blue paper</em>, 2013; 1–109.</p></div></div><p>Various systems will be developed and deployed as technology advances. Following how these systems are implemented and adapted to current healthcare challenges will be interesting. What is clear is that AI systems that make predictions or assist with decision-making cannot be viewed simply in a transactional sense; they are not like calculators involving a simple input and output; it is an interaction that consists of a relationship between the user and the patient (both humans) and the machine. As mentioned above, the success of these systems will be dependent on various patient characteristics but also on how AI relates to individual practitioners. This is about knowing if the tool produces the correct response in the correct setting. In this way, the success of the AI will depend on the practitioner’s experience just as much as its ability to interpret the ultrasound machine’s output. In one scenario, an expert sonographer may quickly identify an error in the AI output, whereas a student or newly qualified sonographer may take the output on trust. AI interactions will require a delicate balance between trust and scepticism. Successfully integrating AI into sonography will mean navigating this new reality.</p>
</section>
<section id="sonography-education-balancing-ai-tools-and-integrity" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sonography-education-balancing-ai-tools-and-integrity">Sonography Education – Balancing AI Tools and Integrity</h2>
<p>The impacts of AI in the sonography profession are likely to be gradual. However, their effect on education, including sonography education, has more immediate implications. Generative AI (GenAI), the term used to describe AI that can create new content, is capable of completing many student tasks with high accuracy, making it one of the most disruptive technologies ever seen in education. The latest ChatGPT release, o1-preview, designed explicitly for advanced logic and reasoning, is reported to perform at the PhD level<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;OpenAI. Introducing OpenAI o1-preview. <a href="https://openai.com/index/introducing-openai-o1-preview/" class="uri">https://openai.com/index/introducing-openai-o1-preview/</a></p></div><div id="fn5"><p><sup>5</sup>&nbsp;Parliament of Australia. Inquiry into the use of generative artificial intelligence in the Australian education system ‘Study Buddy or Influencer’. 2024 <a href="https://www.aph.gov.au/Parliamentary_Business/Committees/House/Employment_Education_and_Training/AIineducation/Report" class="uri">https://www.aph.gov.au/Parliamentary_Business/Committees/House/Employment_Education_and_Training/AIineducation/Report</a></p></div><div id="fn6"><p><sup>6</sup>&nbsp;Tertiary Education Quality and Standards Agency. Artificial intelligence request for information – next steps. 2024.<a href="https://www.teqsa.gov.au/about-us/news-and-events/latest-news/artificial-intelligence-request-information-next-steps" class="uri">https://www.teqsa.gov.au/about-us/news-and-events/latest-news/artificial-intelligence-request-information-next-steps</a></p></div></div><p>Multiple authorities are wrestling with the impacts. A parliamentary senate inquiry report, ‘Study Buddy or Influencer’, was released in August 2024, recommending GenAI in education be made a national priority<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. The Tertiary Education Quality and Standards Agency (TEQSA), the national regulatory body of universities, has requested urgent action, asking all institutions to report on how they manage GenAI, particularly how they address the risk of GenAI to award integrity<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>
<p>Universities themselves also understand the reputational risks and potential for significant community backlash, for example, graduates from a particular university who enter the workforce without the competence to perform the job effectively and safely. This is especially critical in fields like sonography and other health degrees, where universities self-certify their students. Once they graduate, there is no post-qualification check to ensure that the skills they reportedly gained meet community expectations.</p>
<p>With these considerations in mind and TEQSA having one eye firmly on risk, an urgent overhaul of assessment practices is underway. Of course, some assessments are designed to assist and promote learning, and AI tools will become welcome additions. Other, high-stakes assessments designed to verify student learning, which directly leads to certifying a degree like sonography, will require rigorous direct supervision – for example, a return of traditional hall-style written exams to verify knowledge acquisition, oral assessment tasks to demonstrate understanding and showcase communication skills, and face-to-face practical evaluations to demonstrate technical skills.</p>
<p>Many online assessment methods developed during COVID-19 are now obsolete. Numerous tips and YouTube videos are circulating on how students can use AI tools during online assessments, even when the assessor is present via Teams or Zoom. Some may have already seen videos of job candidates using AI tools to convert an employer’s questions into text responses displayed on a hidden monitor during interviews.</p>
<p>Risks must be countered with rewards. In the professional world, it is not unreasonable to expect the question of why you used an AI tool to become ‘why not’, especially when there are workflow and efficiency gains and improvements to overall patient care. Therefore, it is incumbent on educators to encourage the use of tools for relevant tasks. In the sonography context, this might be written communication tasks where students produce artefacts (reports, oral case presentations, ePoster) to showcase and share their knowledge with colleagues.</p>
<p>The use of AI in learning is also an untapped resource. Feedback literacy, an emerging topic in education to improve student learning outcomes, is one area in which AI may assist. The idea focuses not on how supervisors frame feedback for students but on teaching students to respond positively and integrate feedback into their practice. Feedback comes in various forms; sometimes, it is for evaluation; other times, it is an aid to help the student improve; and at different times, it is for praise on a job well done. All of which are important for growth. How students manage their emotional responses to these three is an important skill to develop. More research is needed, but perhaps AI chatbots may be a good source of feedback literacy development – just a thought.</p>
<p>As AI becomes more integrated into sonography and education, the focus must remain on maintaining the integrity of the profession and upholding community trust in practitioners. While these tools offer potential, ensuring that they enhance rather than replace human judgement and accountability is essential to preserving the core values of patient care and professional competence.</p>


</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/edwchris\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2024, Chris Edwards</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This page is built with ❤️ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>